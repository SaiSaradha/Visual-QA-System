[MLP]
batch_size : 256
num_epochs: 3
hidden_layers: 2
hidden_units: 1024
dropout: 0.5
dim_im: 4096
dim_txt: 300
print_loss: 100
learning_rate: 0.001

[LSTM]
batch_size : 512
num_epochs: 100
hl_mlp: 3
hu_mlp: 1024
hl_lstm:1
hu_lstm:512
drop_lstm:0.5
max_que_len: 30

[ATTN]
batch_size : 256
num_epochs: 200
hidden_layers: 2
hidden_units: 1024

[MCB]
batch_size : 256
num_epochs: 200
hidden_layers: 2
hidden_units: 1024